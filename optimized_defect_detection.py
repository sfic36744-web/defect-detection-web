# optimized_defect_detection.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.models.segmentation import deeplabv3_resnet50
from torchvision import models
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import os
import cv2
from datetime import datetime
import glob
import random

print("ğŸš€ Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ´Ù Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¹Ù† Ø§Ù„Ø¹ÙŠÙˆØ¨ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ÙØ­Ø³Ù†Ø© ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø±Ø©")
print("=" * 60)

# âœ… ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¨Ø°ÙˆØ± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚
def set_seeds(seed=42):
    """ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¨Ø°ÙˆØ± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ³Ù‚Ø©"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seeds(42)  # âœ… ØªØ·Ø¨ÙŠÙ‚ ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¨Ø°ÙˆØ±

# âœ… Ø§Ù„Ø¬Ù‡Ø§Ø²
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"âš™ï¸ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {device}")

# âœ… ØªØ¹Ø±ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ Segmentation
class SegmentationModel(nn.Module):
    def __init__(self, num_classes=2):
        super(SegmentationModel, self).__init__()
        self.model = deeplabv3_resnet50(pretrained=False, num_classes=21, aux_loss=True)
        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
        if hasattr(self.model, 'aux_classifier') and self.model.aux_classifier is not None:
            self.model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)
    
    def forward(self, x):
        return self.model(x)['out']

# âœ… ØªØ¹Ø±ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚
class CompatibleClassificationModel(nn.Module):
    def __init__(self, num_classes=4):
        super(CompatibleClassificationModel, self).__init__()
        self.backbone = models.resnet50(pretrained=False)
        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)
    
    def forward(self, x):
        return self.backbone(x)

# âœ… ÙØ¦Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
CLASS_NAMES = ['Ø³Ù„ÙŠÙ…Ø©', 'Ø«Ù‚Ø¨', 'Ø´Ø§Ù‚ÙˆÙ„ÙŠ', 'Ø§ÙÙ‚ÙŠ']
DEFECT_THRESHOLD = 0.05  # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹ØªØ¨Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø°Ø¨Ø©

# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
def load_models():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    models_loaded = {}
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Segmentation
    print("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Segmentation...")
    try:
        seg_checkpoint = torch.load('best_high_accuracy_model.pth', map_location=device, weights_only=False)
        seg_model = SegmentationModel(num_classes=2).to(device)
        
        seg_state_dict = seg_checkpoint['model_state_dict']
        filtered_seg_dict = {}
        for key, value in seg_state_dict.items():
            if key in seg_model.state_dict() and seg_model.state_dict()[key].shape == value.shape:
                filtered_seg_dict[key] = value
        
        seg_model.load_state_dict(filtered_seg_dict, strict=False)
        seg_model.eval()
        models_loaded['segmentation'] = seg_model
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Segmentation Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Segmentation: {e}")
        return None
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ
    print("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ...")
    try:
        cls_checkpoint = torch.load('best_classification_model.pth', map_location=device, weights_only=False)
        cls_model = CompatibleClassificationModel(num_classes=4).to(device)
        cls_model.load_state_dict(cls_checkpoint, strict=False)
        cls_model.eval()
        models_loaded['classification'] = cls_model
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ù†Ø¬Ø§Ø­")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {e}")
        print("âš ï¸ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Segmentation ÙÙ‚Ø·")
        models_loaded['classification'] = None
    
    return models_loaded

# âœ… ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ±
def get_transforms():
    """Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ±"""
    seg_transform = transforms.Compose([
        transforms.Resize((400, 400)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    cls_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    return seg_transform, cls_transform

# âœ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØµÙ†ÙŠÙ
def predict_classification(model, image, transform):
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù†ÙˆØ¹ Ø§Ù„Ø¹ÙŠØ¨"""
    if model is None:
        return None, None
    
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
    
    return CLASS_NAMES[predicted_class], confidence

# âœ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØ¬Ø²Ø¦Ø©
def predict_segmentation(model, image, transform):
    """Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¹ÙŠØ¨"""
    original_size = image.size
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        output = model(image_tensor)
        prediction = torch.argmax(output, dim=1).squeeze().cpu().numpy()
    
    prediction_resized = np.array(Image.fromarray(prediction.astype(np.uint8)).resize(original_size, Image.NEAREST))
    
    return prediction_resized

# âœ… ØªØ­Ù„ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Segmentation
def analyze_segmentation_results(prediction_mask, original_image):
    """ØªØ­Ù„ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù€ Segmentation"""
    defect_pixels = np.sum(prediction_mask == 1)
    total_pixels = prediction_mask.size
    defect_percentage = (defect_pixels / total_pixels) * 100
    
    # âœ… ØªØ·Ø¨ÙŠÙ‚ Ù…Ø±Ø´Ø­ Ø¥Ø¶Ø§ÙÙŠ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„ØµØºÙŠØ±Ø©
    kernel = np.ones((3, 3), np.uint8)
    cleaned_mask = cv2.morphologyEx(prediction_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)
    
    contours, _ = cv2.findContours(
        cleaned_mask, 
        cv2.RETR_EXTERNAL, 
        cv2.CHAIN_APPROX_SIMPLE
    )
    
    defects_info = []
    for i, contour in enumerate(contours):
        area = cv2.contourArea(contour)
        # âœ… Ø²ÙŠØ§Ø¯Ø© Ø¹ØªØ¨Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø®Ø§Ø·Ø¦
        if area > 10:  # Ø²ÙŠØ§Ø¯Ø© Ù…Ù† 10 Ø¥Ù„Ù‰ 50
            x, y, w, h = cv2.boundingRect(contour)
            defect_percent = (area / total_pixels) * 100
            
            # ØªØ­Ù„ÙŠÙ„ Ø´ÙƒÙ„ Ø§Ù„Ø¹ÙŠØ¨
            aspect_ratio = w / h if h > 0 else 0
            
            # âœ… ØªØ­Ø³ÙŠÙ† ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ù…Ø¹ Ø¹ØªØ¨Ø§Øª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
            if aspect_ratio > 1.0:  # Ø²ÙŠØ§Ø¯Ø© Ù…Ù† 2 Ø¥Ù„Ù‰ 3
                shape_type = "Ø®Ø· Ø£ÙÙ‚ÙŠ"
                estimated_class = "Ø§ÙÙ‚ÙŠ"
            elif aspect_ratio < 0.33:  # ØªØºÙŠÙŠØ± Ù…Ù† 0.5 Ø¥Ù„Ù‰ 0.33
                shape_type = "Ø®Ø· Ø¹Ù…ÙˆØ¯ÙŠ"
                estimated_class = "Ø´Ø§Ù‚ÙˆÙ„ÙŠ"
            elif 0.8 <= aspect_ratio <= 1.2:  # Ù†Ø·Ø§Ù‚ Ø£Ø¶ÙŠÙ‚ Ù„Ù„Ø«Ù‚ÙˆØ¨
                shape_type = "Ø«Ù‚Ø¨ Ø¯Ø§Ø¦Ø±ÙŠ"
                estimated_class = "Ø«Ù‚Ø¨"
            else:
                shape_type = "Ø¹ÙŠØ¨ ØºÙŠØ± Ù…Ù†ØªØ¸Ù…"
                estimated_class = "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            
            defects_info.append({
                'id': i + 1,
                'bbox': (x, y, w, h),
                'area': area,
                'defect_percentage': defect_percent,
                'center': (x + w//2, y + h//2),
                'shape_type': shape_type,
                'aspect_ratio': aspect_ratio,
                'estimated_class': estimated_class
            })
    
    return defect_percentage, defects_info

# âœ… Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙˆØ¨ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¹ÙŠØ¨
def create_default_defects(defect_type, image_size):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙˆØ¨ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒØªØ´Ù Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹ÙŠØ¨Ø§Ù‹ ÙˆÙ„ÙƒÙ† Segmentation Ù„Ø§ ÙŠØ¬Ø¯"""
    width, height = image_size
    
    default_defects_map = {
        "Ø´Ø§Ù‚ÙˆÙ„ÙŠ": [
            {
                'id': 1,
                'bbox': (width//2 - 5, height//4, 10, height//2),
                'area': 10 * (height//2),
                'defect_percentage': 0.8,
                'center': (width//2, height//2),
                'shape_type': "Ø®Ø· Ø¹Ù…ÙˆØ¯ÙŠ",
                'aspect_ratio': 0.05,
                'estimated_class': "Ø´Ø§Ù‚ÙˆÙ„ÙŠ"
            }
        ],
        "Ø§ÙÙ‚ÙŠ": [
            {
                'id': 1,
                'bbox': (width//4, height//2 - 5, width//2, 10),
                'area': (width//2) * 10,
                'defect_percentage': 0.7,
                'center': (width//2, height//2),
                'shape_type': "Ø®Ø· Ø£ÙÙ‚ÙŠ",
                'aspect_ratio': 20.0,
                'estimated_class': "Ø§ÙÙ‚ÙŠ"
            }
        ],
        "Ø«Ù‚Ø¨": [
            {
                'id': 1,
                'bbox': (width//2 - 15, height//2 - 15, 30, 30),
                'area': 700,
                'defect_percentage': 0.5,
                'center': (width//2, height//2),
                'shape_type': "Ø«Ù‚Ø¨ Ø¯Ø§Ø¦Ø±ÙŠ",
                'aspect_ratio': 1.0,
                'estimated_class': "Ø«Ù‚Ø¨"
            }
        ]
    }
    
    return default_defects_map.get(defect_type, [])

# âœ… Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ù…Ù†Ø·Ù‚ Ø£ÙƒØ«Ø± ØªØ­ÙØ¸Ø§Ù‹
def intelligent_result_fusion(classification_result, segmentation_result, image_size):
    """Ø¯Ù…Ø¬ Ù…Ø­Ø³Ù† Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±"""
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    cls_type, cls_confidence, cls_validation = classification_result
    seg_percentage, seg_defects, seg_validation = segmentation_result
    
    print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù…Ø¬:")
    print(f"   - Ø§Ù„ØªØµÙ†ÙŠÙ: {cls_type} (Ø«Ù‚Ø©: {cls_confidence})")
    print(f"   - Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹ÙŠÙˆØ¨: {seg_percentage:.4f}%")
    print(f"   - Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙˆØ¨: {len(seg_defects)}")
    
    # âœ… Ø¹ØªØ¨Ø§Øª Ø£ÙƒØ«Ø± ØªØ­ÙØ¸Ø§Ù‹
    HIGH_CONFIDENCE_THRESHOLD = 0.3  # Ø²ÙŠØ§Ø¯Ø© Ù…Ù† 0.3 Ø¥Ù„Ù‰ 0.7
    MEDIUM_CONFIDENCE_THRESHOLD = 0.5
    
    # âœ… Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
    if cls_type and cls_type != "Ø³Ù„ÙŠÙ…Ø©" and cls_confidence > HIGH_CONFIDENCE_THRESHOLD:
        print(f"   ğŸ“¢ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§ÙƒØªØ´Ù Ø¹ÙŠØ¨ Ø¨Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©: {cls_type}")
        
        if float(seg_percentage) < DEFECT_THRESHOLD:
            print("   âš ï¸  Segmentation Ù„Ù… ÙŠÙƒØªØ´Ù Ø¹ÙŠÙˆØ¨ØŒ Ù„ÙƒÙ† Ù†Ø«Ù‚ Ø¨Ø§Ù„ØªØµÙ†ÙŠÙ (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)")
            default_percentages = {
                "Ø«Ù‚Ø¨": 0.5,
                "Ø´Ø§Ù‚ÙˆÙ„ÙŠ": 0.8,
                "Ø§ÙÙ‚ÙŠ": 0.7
            }
            default_percentage = default_percentages.get(cls_type, 0.5)
            default_defects = create_default_defects(cls_type, image_size)
            
            return cls_type, cls_confidence, "ØªÙ… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙ Ø°Ùˆ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©", default_percentage, default_defects
        else:
            return cls_type, cls_confidence, "Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ù€ Segmentation", seg_percentage, seg_defects
    
    # âœ… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø«Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ù…ØªÙˆØ³Ø·Ø©ØŒ Ù†ØªØ­Ù‚Ù‚ Ù…Ù† Segmentation
    elif cls_type and cls_type != "Ø³Ù„ÙŠÙ…Ø©" and cls_confidence > MEDIUM_CONFIDENCE_THRESHOLD:
        print(f"   ğŸ“¢ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§ÙƒØªØ´Ù Ø¹ÙŠØ¨ Ø¨Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©: {cls_type}")
        
        # Ù†Ø«Ù‚ Ø¨Ø§Ù„ØªØµÙ†ÙŠÙ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ£ÙƒÙŠØ¯ Ù…Ù† Segmentation
        if float(seg_percentage) >= DEFECT_THRESHOLD and seg_defects:
            return cls_type, (cls_confidence + 0.1), "Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¤ÙƒØ¯Ø© Ø¨Ù€ Segmentation", seg_percentage, seg_defects
        else:
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¤ÙƒØ¯ SegmentationØŒ Ù†Ø¹ÙŠØ¯ Ø§Ù„Ù†Ø¸Ø±
            print("   âš ï¸  Segmentation Ù„Ù… ÙŠØ¤ÙƒØ¯ Ø§Ù„Ø¹ÙŠØ¨ØŒ Ù†Ø¹ÙŠØ¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…")
    
    # âœ… Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù€ Segmentation Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† ÙˆØ§Ø¶Ø­Ø§Ù‹
    if float(seg_percentage) >= DEFECT_THRESHOLD and seg_defects:
        print(f"   ğŸ“¢ Segmentation Ø§ÙƒØªØ´Ù {len(seg_defects)} Ø¹ÙŠØ¨ Ø¨ÙˆØ¶ÙˆØ­")
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ù…Ù† Segmentation
        defect_types = [d.get('estimated_class', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯') for d in seg_defects]
        if defect_types:
            from collections import Counter
            type_counts = Counter(defect_types)
            most_common = type_counts.most_common(1)[0]
            estimated_type = most_common[0]
            
            # âœ… Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹ÙŠÙˆØ¨ ÙˆØ§ØªØ³Ø§Ù‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            base_confidence = min(0.7 + (float(seg_percentage) / 100), 0.9)
            
            # âœ… Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ³Ù‚Ø©
            if cls_type == estimated_type:
                base_confidence += 0.1
            elif cls_type == "Ø³Ù„ÙŠÙ…Ø©":
                base_confidence -= 0.1
            
            return f"{estimated_type}", max(0.6, min(0.95, base_confidence)), "ØªÙ… Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ù…Ù† Segmentation", seg_percentage, seg_defects
    
    # âœ… Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØµÙ†ÙŠÙ ÙŠÙ‚ÙˆÙ„ Ø³Ù„ÙŠÙ…Ø© ÙˆSegmentation Ù„Ø§ ÙŠØ¬Ø¯ Ø¹ÙŠÙˆØ¨
    if cls_type == "Ø³Ù„ÙŠÙ…Ø©" and float(seg_percentage) < DEFECT_THRESHOLD:
        confidence = max(cls_confidence if cls_confidence else 0.9, 0.85)
        return "Ø³Ù„ÙŠÙ…Ø©", confidence, "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹ÙŠÙˆØ¨ Ù…ÙƒØªØ´ÙØ©", seg_percentage, seg_defects
    
    # âœ… Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© - Ø³Ù„ÙŠÙ…Ø© Ù…Ø¹ Ø«Ù‚Ø© Ù…Ø¹ØªØ¯Ù„Ø©
    return "Ø³Ù„ÙŠÙ…Ø©", 0.75, "Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¹ÙŠÙˆØ¨ Ù…Ø¤ÙƒØ¯Ø©", seg_percentage, seg_defects

# âœ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…Ø±Ø¦ÙŠ Ù…Ø­Ø³Ù†
def create_enhanced_visual_report(original_image, segmentation_mask, defect_type, confidence, 
                                defect_percentage, defects_info, output_path, fusion_reason=""):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…Ø±Ø¦ÙŠ Ù…Ø­Ø³Ù† Ù…Ø¹ Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ø¹ÙŠÙˆØ¨"""
    try:
        original_array = np.array(original_image)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹ÙŠÙˆØ¨
        result_image = original_array.copy()
        
        # Ø±Ø³Ù… bounding boxes Ø­ÙˆÙ„ Ø§Ù„Ø¹ÙŠÙˆØ¨
        for defect in defects_info:
            x, y, w, h = defect['bbox']
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ÙˆÙ† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            color_map = {
                "Ø§ÙÙ‚ÙŠ": (255, 165, 0),    # Ø¨Ø±ØªÙ‚Ø§Ù„ÙŠ
                "Ø´Ø§Ù‚ÙˆÙ„ÙŠ": (0, 255, 255),  # Ø³Ù…Ø§ÙˆÙŠ
                "Ø«Ù‚Ø¨": (255, 0, 0),       # Ø£Ø­Ù…Ø±
                "ØºÙŠØ± Ù…Ø­Ø¯Ø¯": (128, 0, 128) # Ø¨Ù†ÙØ³Ø¬ÙŠ
            }
            
            color = color_map.get(defect['estimated_class'], (0, 255, 0))
            label = f"{defect['estimated_class']} {defect['id']}"
            
            # Ø±Ø³Ù… Ø§Ù„Ù…Ø³ØªØ·ÙŠÙ„ Ø­ÙˆÙ„ Ø§Ù„Ø¹ÙŠØ¨
            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 3)
            
            # Ø±Ø³Ù… Ø®Ù„ÙÙŠØ© Ù„Ù„Ù†Øµ
            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(result_image, (x, y-30), (x + text_size[0] + 10, y), color, -1)
            
            # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ
            cv2.putText(result_image, label, (x+5, y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # Ø±Ø³Ù… Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø±ÙƒØ²
            center_x, center_y = defect['center']
            cv2.circle(result_image, (center_x, center_y), 5, color, -1)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø´ÙƒÙ„
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        fig.suptitle('ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ´Ù Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¹Ù† Ø§Ù„Ø¹ÙŠÙˆØ¨', fontsize=18, fontweight='bold', y=0.95)
        
        # Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        axes[0, 0].imshow(original_array)
        axes[0, 0].set_title('ğŸ“· Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©', fontsize=14, fontweight='bold', pad=20)
        axes[0, 0].axis('off')
        
        # Ø®Ø±ÙŠØ·Ø© Segmentation
        im = axes[0, 1].imshow(segmentation_mask, cmap='hot')
        axes[0, 1].set_title(f'ğŸ—ºï¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹ÙŠÙˆØ¨ - {defect_percentage:.4f}%', fontsize=14, fontweight='bold', pad=20)
        axes[0, 1].axis('off')
        plt.colorbar(im, ax=axes[0, 1], fraction=0.046)
        
        # Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹ÙŠÙˆØ¨
        axes[1, 0].imshow(result_image)
        axes[1, 0].set_title('ğŸ¯ ØªØ­Ø¯ÙŠØ¯ Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¹ÙŠÙˆØ¨', fontsize=14, fontweight='bold', pad=20)
        axes[1, 0].axis('off')
        
        # ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        axes[1, 1].axis('off')
        
        confidence_text = f"{confidence*100:.1f}%" if confidence else "ØºÙŠØ± Ù…ØªÙˆÙØ±"
        
        report_text = f"""
        ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„:
        
        ğŸ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù†ØªØ¬: {defect_type}
        ğŸ“ˆ Ø«Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ: {confidence_text}
        ğŸ” Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¯Ù…Ø¬: {fusion_reason}
        
        ğŸ“ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙŠÙˆØ¨:
        â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø¹ÙŠØ¨Ø©: {defect_percentage:.4f}%
        â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(defects_info)}
        â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª Ø§Ù„Ù…Ø¹ÙŠØ¨Ø©: {np.sum(segmentation_mask == 1):,}
        
        ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¹ÙŠÙˆØ¨:
        """
        
        for defect in defects_info:
            report_text += f"    â€¢ Ø¹ÙŠØ¨ {defect['id']}: {defect['shape_type']}\n"
            report_text += f"      ğŸ“ Ø§Ù„Ù…Ø³Ø§Ø­Ø©: {defect['area']} Ø¨ÙŠÙƒØ³Ù„ ({defect['defect_percentage']:.4f}%)\n"
            report_text += f"      ğŸ“ Ø§Ù„Ù…ÙˆÙ‚Ø¹: ({defect['center'][0]}, {defect['center'][1]})\n"
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©
        if defect_type == 'Ø³Ù„ÙŠÙ…Ø©':
            quality_status = "âœ… Ù…Ù†ØªØ¬ Ø³Ù„ÙŠÙ… - Ù…Ù‚Ø¨ÙˆÙ„"
            color = "green"
        else:
            quality_status = "âŒ Ù…Ù†ØªØ¬ Ù…Ø¹ÙŠØ¨ - Ù…Ø±ÙÙˆØ¶"
            color = "red"
        
        report_text += f"\nğŸ¯ Ù‚Ø±Ø§Ø± Ø§Ù„Ø¬ÙˆØ¯Ø©:\n    {quality_status}"
        
        axes[1, 1].text(0.05, 0.95, report_text, transform=axes[1, 1].transAxes, 
                       fontsize=11, verticalalignment='top', color=color, fontweight='bold',
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)
        
        # Ø­ÙØ¸ ØµÙˆØ±Ø© Ù…Ù†ÙØµÙ„Ø© Ù…Ø¹ Ø§Ù„Ø¹ÙŠÙˆØ¨ ÙÙ‚Ø·
        result_image_path = output_path.replace('.png', '_with_defects.png')
        plt.figure(figsize=(12, 8))
        plt.imshow(result_image)
        plt.title(f'ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹ÙŠÙˆØ¨ - {defect_type}', fontsize=16, fontweight='bold', pad=20)
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(result_image_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        return output_path, result_image_path
    
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø±Ø¦ÙŠ: {e}")
        plt.close('all')
        return None, None

# âœ… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
def intelligent_defect_detection(image_path, output_dir="defect_reports"):
    """Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø¹ÙŠÙˆØ¨ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©"""
    print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {os.path.basename(image_path)}")
    
    # âœ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¨Ø°ÙˆØ± Ù‚Ø¨Ù„ ÙƒÙ„ ØªØ­Ù„ÙŠÙ„ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚
    set_seeds(42)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    models = load_models()
    if models is None or 'segmentation' not in models:
        print("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬!")
        return None
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
    try:
        original_image = Image.open(image_path).convert('RGB')
        print(f"ğŸ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {original_image.size}")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {e}")
        return None
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ±
    seg_transform, cls_transform = get_transforms()
    
    # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØµÙ†ÙŠÙ
    defect_type, confidence = predict_classification(
        models.get('classification'), original_image, cls_transform
    )
    
    # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„ØªØ¬Ø²Ø¦Ø©
    segmentation_mask = predict_segmentation(
        models['segmentation'], original_image, seg_transform
    )
    
    # ØªØ­Ù„ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Segmentation
    defect_percentage, defects_info = analyze_segmentation_results(
        segmentation_mask, original_image
    )
    
    # ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    cls_validation = "Ù†ØªÙŠØ¬Ø© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø©" if confidence and confidence > 0.7 else "Ù†ØªÙŠØ¬Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø§Ù„Ø«Ù‚Ø©"
    seg_validation = "ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¹ÙŠÙˆØ¨" if defect_percentage >= DEFECT_THRESHOLD else "Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¹ÙŠÙˆØ¨"
    
    print(f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©:")
    if defect_type:
        print(f"   â€¢ Ø§Ù„ØªØµÙ†ÙŠÙ: {defect_type} (Ø«Ù‚Ø©: {confidence*100:.1f}%) - {cls_validation}")
    print(f"   â€¢ Segmentation: {defect_percentage:.4f}% Ø¹ÙŠÙˆØ¨ - {seg_validation}")
    print(f"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙˆØ¨: {len(defects_info)}")
    
    # Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ ØªÙ…Ø±ÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø©
    final_type, final_confidence, fusion_reason, effective_defect_percentage, effective_defects_info = intelligent_result_fusion(
        (defect_type, confidence, cls_validation), 
        (defect_percentage, defects_info, seg_validation),
        original_image.size
    )
    
    print(f"ğŸ§  Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ:")
    print(f"   â€¢ Ø§Ù„Ø­Ø§Ù„Ø©: {final_type}")
    print(f"   â€¢ Ø§Ù„Ø«Ù‚Ø©: {final_confidence:.2f}")
    print(f"   â€¢ Ø§Ù„Ø³Ø¨Ø¨: {fusion_reason}")
    print(f"   â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙØ¹Ø§Ù„Ø©: {effective_defect_percentage:.4f}%")
    print(f"   â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„ÙØ¹Ø§Ù„: {len(effective_defects_info)}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
    os.makedirs(output_dir, exist_ok=True)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"defect_report_{timestamp}.png"
    output_path = os.path.join(output_dir, output_filename)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø±Ø¦ÙŠ
    report_path, result_image_path = create_enhanced_visual_report(
        original_image, 
        segmentation_mask, 
        final_type, 
        final_confidence,
        effective_defect_percentage,
        effective_defects_info, 
        output_path,
        fusion_reason
    )
    
    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_path}")
    print(f"ğŸ–¼ï¸ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ø§Ù„Ø¹ÙŠÙˆØ¨ ÙÙŠ: {result_image_path}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print(f"\nğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:")
    print(f"   ğŸ¯ Ø§Ù„Ø­Ø§Ù„Ø©: {final_type}")
    print(f"   ğŸ“ˆ Ø§Ù„Ø«Ù‚Ø©: {final_confidence*100:.1f}%")
    print(f"   ğŸ” Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹ÙŠÙˆØ¨: {effective_defect_percentage:.4f}%")
    print(f"   ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙˆØ¨: {len(effective_defects_info)}")
    print(f"   ğŸ’¡ Ø³Ø¨Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø±: {fusion_reason}")
    
    if effective_defects_info:
        print(f"   ğŸ¨ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
        for defect in effective_defects_info:
            print(f"      - Ø¹ÙŠØ¨ {defect['id']}: {defect['shape_type']}")
            print(f"        ğŸ“ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {defect['center']}, ğŸ“ Ø§Ù„Ù…Ø³Ø§Ø­Ø©: {defect['area']} Ø¨ÙŠÙƒØ³Ù„")
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    if final_type == 'Ø³Ù„ÙŠÙ…Ø©':
        print("   âœ… Ù‚Ø±Ø§Ø± Ø§Ù„Ø¬ÙˆØ¯Ø©: Ù…Ù†ØªØ¬ Ø³Ù„ÙŠÙ… - Ù…Ù‚Ø¨ÙˆÙ„")
    else:
        print("   âŒ Ù‚Ø±Ø§Ø± Ø§Ù„Ø¬ÙˆØ¯Ø©: Ù…Ù†ØªØ¬ Ù…Ø¹ÙŠØ¨ - Ù…Ø±ÙÙˆØ¶")
    
    return {
        'final_type': final_type,
        'final_confidence': final_confidence,
        'fusion_reason': fusion_reason,
        'defect_percentage': effective_defect_percentage,
        'defect_count': len(effective_defects_info),
        'defects_info': effective_defects_info,
        'report_path': report_path,
        'result_image_path': result_image_path
    }

# Ø¨Ø§Ù‚ÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±...
def find_images_automatically():
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† Ø§Ù„ØµÙˆØ±"""
    possible_locations = [".", "./images", "./data"]
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
    found_images = []
    
    for location in possible_locations:
        if os.path.exists(location):
            for ext in image_extensions:
                pattern = os.path.join(location, ext)
                images = glob.glob(pattern)
                found_images.extend(images)
    
    return list(set(found_images))[:5]

def main():
    print("=" * 60)
    print("ğŸ¯ Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ´Ù Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¹Ù† Ø§Ù„Ø¹ÙŠÙˆØ¨ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…ÙØ­Ø³Ù†Ø© ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø±Ø©")
    print("=" * 60)
    
    if not os.path.exists('best_high_accuracy_model.pth'):
        print("âŒ Ù†Ù…ÙˆØ°Ø¬ Segmentation ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return
    
    print("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…")
    
    while True:
        print("\n" + "=" * 50)
        print("1. ğŸ“ ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© Ù…Ø­Ø¯Ø¯Ø©")
        print("2. ğŸ“‚ ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± ÙÙŠ Ù…Ø¬Ù„Ø¯") 
        print("3. ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† Ø§Ù„ØµÙˆØ±")
        print("4. ğŸšª Ø®Ø±ÙˆØ¬")
        
        choice = input("\nØ§Ø®ØªØ± Ø§Ù„Ø®ÙŠØ§Ø± (1-4): ").strip()
        
        if choice == '1':
            print("\nğŸ“ Ø£Ø¯Ø®Ù„ Ù…Ø³Ø§Ø± Ø§Ù„ØµÙˆØ±Ø©:")
            image_path = input().strip().strip('"')
            
            if not image_path or not os.path.exists(image_path):
                print("âŒ Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± ØµØ­ÙŠØ­!")
                continue
            
            intelligent_defect_detection(image_path)
            
        elif choice == '2':
            print("\nğŸ“‚ Ø£Ø¯Ø®Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¬Ù„Ø¯:")
            folder_path = input().strip().strip('"')
            
            if not folder_path or not os.path.exists(folder_path):
                print("âŒ Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± ØµØ­ÙŠØ­!")
                continue
            
            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
            image_files = []
            for ext in image_extensions:
                pattern = os.path.join(folder_path, ext)
                image_files.extend(glob.glob(pattern))
            
            if not image_files:
                print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ±!")
                continue
            
            print(f"\nğŸ“ ÙˆØ¬Ø¯Øª {len(image_files)} ØµÙˆØ±Ø©")
            for i, image_path in enumerate(image_files, 1):
                print(f"\nğŸ”„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {i}/{len(image_files)}")
                intelligent_defect_detection(image_path)
                
        elif choice == '3':
            print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† Ø§Ù„ØµÙˆØ±...")
            found_images = find_images_automatically()
            if found_images:
                print(f"ğŸ“ ÙˆØ¬Ø¯Øª {len(found_images)} ØµÙˆØ±Ø©:")
                for i, img_path in enumerate(found_images, 1):
                    print(f"   {i}. {os.path.basename(img_path)}")
                
                use_all = input("\nÙ‡Ù„ ØªØ±ÙŠØ¯ ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±ØŸ (y/n): ").strip().lower()
                if use_all == 'y':
                    for image_path in found_images:
                        intelligent_defect_detection(image_path)
            else:
                print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹!")
                
        elif choice == '4':
            print("ğŸ‘‹ Ø´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù…!")
            break
        
        else:
            print("âŒ Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­!")

if __name__ == "__main__":
    main()